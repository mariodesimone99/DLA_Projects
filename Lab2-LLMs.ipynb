{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results \n",
    "\n",
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists).\n",
    "\n",
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters.\n",
    "\n",
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3324be",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb4678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707be839",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5c7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "BLOCK_SIZE = 256\n",
    "EPOCHS = 10\n",
    "N_EMBD = 512\n",
    "N_HEAD = 6\n",
    "N_LAYER = 6\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0f908",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e3664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfernoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_size, path='dante_inferno.txt', split='train'):\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = list(text)\n",
    "            if split == 'train':\n",
    "                self.text = text[:int(len(text)*0.9)]\n",
    "            elif split == 'val':\n",
    "                self.text = text[int(len(text)*0.9):]\n",
    "            else:\n",
    "                raise ValueError(\"split must be 'train' or 'val'\")\n",
    "            self.block_size = block_size\n",
    "            self.vocab_size = len(set(self.text))\n",
    "            self.stoi = { ch:i for i,ch in enumerate(set(self.text)) }\n",
    "            self.itos = { i:ch for i,ch in enumerate(set(self.text)) }\n",
    "            self.data = torch.tensor([self.stoi[c] for c in self.text], dtype=torch.long)\n",
    "            self.split = split\n",
    "            self.train_size = int(0.9 * len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size] if idx + self.block_size < len(self.data) else self.data[idx:]\n",
    "        return x\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.itos[i.item()] for i in indices])\n",
    "    \n",
    "def GetBatch(dataset, batch_size, device):\n",
    "    idx = torch.randint(len(dataset), (batch_size,))\n",
    "    x = torch.stack([dataset.data[i:i+BLOCK_SIZE] for i in idx])\n",
    "    y = torch.stack([dataset.data[i+1:i+BLOCK_SIZE+1] for i in idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c245c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256])\n",
      "tensor([21, 15,  0, 55, 24, 24, 22, 48,  0, 25, 47, 53, 40, 27, 53, 25, 40, 21,\n",
      "        15, 53, 45, 52,  2, 31, 40,  2, 40, 22, 48,  0, 25, 47, 53, 40, 27, 53,\n",
      "        25, 40, 27,  0, 45, 42,  0, 25,  2, 31, 24, 40, 40, 27, 53, 25, 40, 21,\n",
      "         0, 45, 52, 48, 15, 12, 40,  2, 40, 27, 53, 25, 40, 27,  2, 25, 25, 12,\n",
      "        40, 47, 12, 40, 27,  0, 26, 21,  2, 36, 36,  0, 31, 24, 40, 40,  2, 40,\n",
      "        27, 53, 25, 40, 27, 53, 26,  2, 40, 25, 53, 26, 21, 15,  0, 36, 12, 40,\n",
      "         2, 40, 27, 53, 25, 40, 12, 26, 21, 15,  0, 25,  2, 55, 24, 24, 25,  2,\n",
      "        38, 40, 35, 12,  0, 54, 40, 27, 53, 25, 40, 26, 12, 54, 40, 47, 12, 41,\n",
      "         2, 15, 26,  0, 40, 27,  2, 25, 25,  0, 45,  2, 36, 36,  0, 24, 40, 40,\n",
      "        27,  0, 41,  0, 36, 12,  2, 15, 40, 41, 12, 47, 12, 40, 45, 48, 53, 41,\n",
      "         2, 15, 40, 25,  2, 38, 40, 42,  2, 47, 53, 25, 12, 31, 24, 40, 40, 25,\n",
      "         2, 38, 40, 25,  0, 41,  2, 40,  0, 40, 26,  2, 35, 25, 53, 40, 47, 12,\n",
      "        40, 21,  2, 15, 15,  0, 40, 53, 40, 47, 12, 40, 26, 21,  2, 36, 36,  0,\n",
      "        18, 24, 24,  4, 53, 12, 40,  0, 25, 47,  0, 41,  0, 45, 40, 27, 53, 25,\n",
      "        40, 36, 12, 40], device='cuda:0')\n",
      "tra;\n",
      "\n",
      "quando con trombe, e quando con campane,\n",
      "  con tamburi e con cenni di castella,\n",
      "  e con cose nostrali e con istrane;\n",
      "\n",
      "ne' gia` con si` diversa cennamella\n",
      "  cavalier vidi muover ne' pedoni,\n",
      "  ne' nave a segno di terra o di stella.\n",
      "\n",
      "Noi andavam con li \n",
      "torch.Size([16, 256])\n",
      "tensor([15,  0, 55, 24, 24, 22, 48,  0, 25, 47, 53, 40, 27, 53, 25, 40, 21, 15,\n",
      "        53, 45, 52,  2, 31, 40,  2, 40, 22, 48,  0, 25, 47, 53, 40, 27, 53, 25,\n",
      "        40, 27,  0, 45, 42,  0, 25,  2, 31, 24, 40, 40, 27, 53, 25, 40, 21,  0,\n",
      "        45, 52, 48, 15, 12, 40,  2, 40, 27, 53, 25, 40, 27,  2, 25, 25, 12, 40,\n",
      "        47, 12, 40, 27,  0, 26, 21,  2, 36, 36,  0, 31, 24, 40, 40,  2, 40, 27,\n",
      "        53, 25, 40, 27, 53, 26,  2, 40, 25, 53, 26, 21, 15,  0, 36, 12, 40,  2,\n",
      "        40, 27, 53, 25, 40, 12, 26, 21, 15,  0, 25,  2, 55, 24, 24, 25,  2, 38,\n",
      "        40, 35, 12,  0, 54, 40, 27, 53, 25, 40, 26, 12, 54, 40, 47, 12, 41,  2,\n",
      "        15, 26,  0, 40, 27,  2, 25, 25,  0, 45,  2, 36, 36,  0, 24, 40, 40, 27,\n",
      "         0, 41,  0, 36, 12,  2, 15, 40, 41, 12, 47, 12, 40, 45, 48, 53, 41,  2,\n",
      "        15, 40, 25,  2, 38, 40, 42,  2, 47, 53, 25, 12, 31, 24, 40, 40, 25,  2,\n",
      "        38, 40, 25,  0, 41,  2, 40,  0, 40, 26,  2, 35, 25, 53, 40, 47, 12, 40,\n",
      "        21,  2, 15, 15,  0, 40, 53, 40, 47, 12, 40, 26, 21,  2, 36, 36,  0, 18,\n",
      "        24, 24,  4, 53, 12, 40,  0, 25, 47,  0, 41,  0, 45, 40, 27, 53, 25, 40,\n",
      "        36, 12, 40, 47], device='cuda:0')\n",
      "ra;\n",
      "\n",
      "quando con trombe, e quando con campane,\n",
      "  con tamburi e con cenni di castella,\n",
      "  e con cose nostrali e con istrane;\n",
      "\n",
      "ne' gia` con si` diversa cennamella\n",
      "  cavalier vidi muover ne' pedoni,\n",
      "  ne' nave a segno di terra o di stella.\n",
      "\n",
      "Noi andavam con li d\n"
     ]
    }
   ],
   "source": [
    "dataset_train = InfernoDataset(BLOCK_SIZE, path='dante_inferno.txt', split='train')\n",
    "dataset_val = InfernoDataset(BLOCK_SIZE, path='dante_inferno.txt', split='val')\n",
    "x, y = GetBatch(dataset_train, BATCH_SIZE, device=DEVICE)\n",
    "print(x.shape)\n",
    "print(x[0, :])\n",
    "print(dataset_train.decode(x[0, :]))\n",
    "print(y.shape)\n",
    "print(y[0, :])\n",
    "print(dataset_train.decode(y[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735dea6d",
   "metadata": {},
   "source": [
    "## Toy GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637160d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, mid_features, out_features, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, mid_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_features, out_features),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention_Head(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_k, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, channels, time-step)\n",
    "        # output of size (batch, time-step, d_k)\n",
    "        k = self.key(x)   # (B, T, d_k)\n",
    "        q = self.query(x) # (B, T, d_k)\n",
    "\n",
    "        w = torch.matmul(q, k.transpose(-2, -1)) # (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n",
    "        w = w / (self.d_k**0.5)\n",
    "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
    "        w = self.dropout(w) # (B, T, T)\n",
    "        v = self.value(x) # (B, T, d_k)\n",
    "        out = w @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, d_k, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Attention_Head(d_model, d_k) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(d_k * num_heads, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, d_k * num_heads)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, mod, size):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.ln = nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.mod(self.ln(x))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.2):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.multi_att = AddAndNorm(MultiHeadAttention(n_head, n_embd, head_size, dropout), n_embd)\n",
    "        self.ffwd = AddAndNorm(FeedForward(n_embd, dropout), n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.multi_att(x)\n",
    "        out = self.ffwd(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, dict_size, n_embd, block_size, n_layer, n_head, device, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(dict_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, dropout=dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) \n",
    "        self.lm_head = LinearLayer(n_embd, n_embd, dict_size, dropout=dropout)\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        return logits #, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93bc720",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b4a9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.333689 M parameters\n",
      "Epoch 0: train_loss = 4.0546, val_loss = 4.0658\n",
      "Epoch 50: train_loss = 2.7874, val_loss = 4.8281\n",
      "Epoch 100: train_loss = 2.7060, val_loss = 5.1059\n",
      "Epoch 150: train_loss = 2.6788, val_loss = 5.3634\n",
      "Epoch 200: train_loss = 2.6494, val_loss = 5.5397\n",
      "Epoch 250: train_loss = 2.6282, val_loss = 5.6641\n",
      "Epoch 300: train_loss = 2.6088, val_loss = 5.7921\n",
      "Epoch 350: train_loss = 2.6633, val_loss = 5.6813\n",
      "Epoch 400: train_loss = 2.6318, val_loss = 5.7574\n",
      "Epoch 450: train_loss = 2.5937, val_loss = 5.8116\n"
     ]
    }
   ],
   "source": [
    "model = GPT(dict_size=dataset_train.vocab_size, n_embd=N_EMBD, block_size=BLOCK_SIZE, n_layer=N_LAYER, n_head=N_HEAD, device=DEVICE)\n",
    "m = model.to(DEVICE)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = F.cross_entropy\n",
    "check = 50\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    x, y = GetBatch(dataset_train, BATCH_SIZE, device=DEVICE)\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits.view(-1, dataset_train.vocab_size), y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % check == 0:\n",
    "        x_val, y_val = GetBatch(dataset_val, BATCH_SIZE, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            logits_val = model(x_val)\n",
    "            loss_val = loss_fn(logits_val.view(-1, dataset_train.vocab_size), y_val.view(-1))\n",
    "            print(f\"Epoch {epoch}: train_loss = {loss.item():.4f}, val_loss = {loss_val.item():.4f}\")\n",
    "torch.save(model.state_dict(), 'gpt_inferno.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2acf251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(dict_size=dataset_train.vocab_size, n_embd=N_EMBD, block_size=BLOCK_SIZE, n_layer=N_LAYER, n_head=N_HEAD, device=DEVICE)\n",
    "model.load_state_dict(torch.load('gpt_inferno.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea857ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "aaaaadaa,ascalaacarbalacalalaquaa aagna asa,aalava palosaQacacasa,zalaaavapabapiappaza <anana,anaganaballsacaa pa a ana a lala`a  dmagoimala  a ialo.agaia va'anauaraga lagasia\n",
      "talasbanalanava a\n",
      "'a\n",
      " 'anaLasa ra tIfa ana e ota la a,a cati Ma\n",
      "a -apa 'XaMala,a dalanala ltanvata a mia\n",
      " \n",
      " amasenarappata'a a'ara ma\n",
      " alta\n",
      "Cra Ganamvara mavamia\n",
      "\n",
      "Tarada la,calgrama, madana M'o,\n",
      "  iarabapalada  liauovadarava ralpedpa  :ala, alam'zila,aba\n",
      "Lalda vzazagvanacgguanta uararalaga ga.\n",
      "\n",
      "\n",
      "cava;a ualagPa e'lla\n",
      "\n",
      "ui cea\n"
     ]
    }
   ],
   "source": [
    "init_context = torch.zeros((1,1), dtype=torch.long, device=DEVICE)\n",
    "max_tokens = 500\n",
    "m.eval()\n",
    "new_text = m.generate(init_context, max_new_tokens=max_tokens)\n",
    "print(\"Generated text:\")\n",
    "print(dataset_train.decode(new_text[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30506687",
   "metadata": {},
   "source": [
    "As we can notice the model does not have good performance, the generated text is chaotic, one thing I noticed is the fact that if I try to train with some other epochs, the model overfits causing the generated text to be composed by a single repeated letter because the output probability distribution is peaked.\n",
    "\n",
    "To reach better performances non-toy models should be used, let's take a look to Hugging Face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55c896",
   "metadata": {},
   "source": [
    "# Hugginface Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d4ab8",
   "metadata": {},
   "source": [
    "Let's try the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   995,  5145]])\n",
      "tensor([[15496,    11,   995]])\n",
      "tensor([[18435,   995]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "print(tokenizer(\"Hello world !\", return_tensors=\"pt\")[\"input_ids\"])\n",
    "print(tokenizer(\"Hello, world\", return_tensors=\"pt\")[\"input_ids\"])\n",
    "print(tokenizer(\" Hello world\", return_tensors=\"pt\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa5316",
   "metadata": {},
   "source": [
    "From this simple example we can estabilish that the tokenizer takes into account primarily the number of words and punctuation, let's try to see a graph summarizing the magnitutde of the compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a869528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_plot(tokenizer, string_list):\n",
    "    x = [len(string_list[i]) for i in range(len(string_list))]\n",
    "    y = [tokenizer(string_list[i], return_tensors=\"pt\")[\"input_ids\"].shape[1] for i in range(len(string_list))]\n",
    "    ratio = [y[i] / x[i] for i in range(len(x))]\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('String Length')\n",
    "    plt.ylabel('Token Count')\n",
    "    plt.title(f'Token Count vs String Length (Ratio = {np.mean(ratio):.2f})')\n",
    "\n",
    "def generate_string_list(model, tokenizer):\n",
    "    string_list = []\n",
    "    model.to(DEVICE)\n",
    "    for i in range(10, 200, 10):\n",
    "        init_context = torch.tensor(i, dtype=torch.long, device=DEVICE).view(1, 1)\n",
    "        generated_text = tokenizer.decode(model.generate(init_context, max_new_tokens=i)[0], skip_special_tokens=True)\n",
    "        string_list.append(generated_text)\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! What does a 'Wargaming 101' look like?\"\n",
      "\n",
      "\"A 'Wargaming 101', like any decent 'mainstream' hobbyist, you'd think. So let's see how you do it…\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "# do_sample controls whether to use sampling or greedy decoding\n",
    "# temperature modulate next token probabilites\n",
    "# top_k is the probability for filtering the vocabulary\n",
    "# top_p condition which tokens to keep for the next step (they must sum up to at least p)\n",
    "new_text_id = model.generate(\n",
    "    input_ids=tokenizer(\"Hello world\", return_tensors=\"pt\")[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "new_text_str = tokenizer.decode(new_text_id[0], skip_special_tokens=True)\n",
    "print(new_text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5379d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "list_strings = generate_string_list(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f27772fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_plot(tokenizer, list_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf1bcb",
   "metadata": {},
   "source": [
    "We can notice at first glance that approximately the number of token grows linaerly with the length of the string, but clearly it is not a fixed rule, as it depends on the kind of string\n",
    "\n",
    "Now we try to play with the temperature parameter of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d31cd2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.1):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and The Last Man on Earth. I'm not sure if I'm going to be able to get through all of them, but I'm going to be able to get through them.\n",
      "\n",
      "I'm not sure if I'm going to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.2):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and The Last Man on Earth. I'm not sure if I'm going to be able to get through all of them all, but I'm sure I'll be able to get through them all.\n",
      "\n",
      "I'm not sure if I'm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.30000000000000004):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and The Great Escape.\n",
      "\n",
      "I'm not sure if I'm going to be able to get through all of them, but I'm sure I'll be able to get through the rest of them.\n",
      "\n",
      "I've been reading a lot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.4):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited and I'm Not A Celebrity. I've been listening to them for years and I'm not sure what to say.\n",
      "\n",
      "I've been listening to them for years and I'm not sure what to say.\n",
      "\n",
      "I'm not a celebrity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.5):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and the album's title track.\n",
      "\n",
      "I also have a very large collection of albums that I've written, but I'm not a fan of the genre. I think the genre is too much of a mess for me to write a book\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.6):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and the underrated, but still a lot of fun, track \"Blackout.\"\n",
      "\n",
      "In the meantime, I'm writing a blog post about this album that you can read here, but I'm also making a special post about this album to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.7000000000000001):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and The Beatles.\n",
      "\n",
      "I was also an avid collector of Bob Dylan and Tom Petty. I have to admit that I had a lot of fun with Bob and the band during their early years. Bob's music was so unique and so beautiful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.8):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and the last album is The Last Wish, which I love.\n",
      "\n",
      "\n",
      "I also like the album title, which I would've thought was a bit more of a homage to the album titles of other albums. If you've ever listened to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "° Generated text (temperature = 0.9):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited and much more. My favorite songs of all time are The Music Never Stopped and I Love You, Don't Let My Friends Down (with all the great songs I have), The World Is Not Enough and even The Best of John Cale.\n",
      "\n",
      "° Generated text (temperature = 1.0):\n",
      "Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited, and many others. My second favorite is my own personal favorite and the best I can share with you is this (on my YouTube channel, at least):\n",
      "\n",
      "The first year my mother came to town was quite a while ago, and I\n"
     ]
    }
   ],
   "source": [
    "T = np.arange(0.1, 1.1, 0.1)\n",
    "context = tokenizer(\"Some of my favorite albums are Kid A, Dark Side of the Moon, The Age of Adz, Highway 61 Revisited\", return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
    "for t in T:\n",
    "    outputs = model.generate(context, max_new_tokens=50, do_sample=True, top_k=50, temperature=t, top_p=0.95)\n",
    "    print(f\"\\n° Generated text (temperature = {t}):\")\n",
    "    print(f\"{tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac6419",
   "metadata": {},
   "source": [
    "As we can see with the growth of the temperature the likelihood of getting repetead phrases decrease, while it seems to be a in increase of the coherence to the topic even though once reached T=1, the model starts adding nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef4ef7",
   "metadata": {},
   "source": [
    "# Exercise 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6792acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f03a3",
   "metadata": {},
   "source": [
    "The code works with both imdb and rotten tomatoes datasets, just change the string below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbb61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_STR = 'rotten_tomatoes' # 'rotten_tomatoes'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "TOKEN_LENGTH = 512\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d85e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'stanfordnlp/imdb' if DATASET_STR == 'imdb' else 'cornell-movie-review-data/rotten_tomatoes'\n",
    "dataset = load_dataset(dataset_path)\n",
    "ds_train = dataset[\"train\"]\n",
    "ds_test = dataset[\"test\"]\n",
    "if DATASET_STR == 'rotten_tomatoes':\n",
    "    ds_val = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2906275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 4265\n",
      "Number of negative samples: 4265\n",
      "\n",
      "Positive example:\n",
      "Text: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "Label: 1\n",
      "\n",
      "Negative example:\n",
      "Text: simplistic , silly and tedious .\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "positive_samples = ds_train.filter(lambda x: x[\"label\"] == 1)\n",
    "negative_samples = ds_train.filter(lambda x: x[\"label\"] == 0)\n",
    "print(f\"Number of positive samples: {len(positive_samples)}\")\n",
    "print(f\"Number of negative samples: {len(negative_samples)}\\n\")\n",
    "\n",
    "print(f\"Positive example:\")\n",
    "print(f\"Text: {positive_samples[0]['text']}\")\n",
    "print(f\"Label: {positive_samples[0]['label']}\\n\")\n",
    "\n",
    "print(f\"Negative example:\")\n",
    "print(f\"Text: {negative_samples[0]['text']}\")\n",
    "print(f\"Label: {negative_samples[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b97f0",
   "metadata": {},
   "source": [
    "### My Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82a396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.input_ids = torch.tensor(tokenized_dataset['text'])\n",
    "        self.labels = torch.tensor(tokenized_dataset['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd6593",
   "metadata": {},
   "source": [
    "To get the advantages of parallelization we need to stack batch of sample every with the same length, of course reviews does not guarantee such constraint (and tokenization too) so the idea I applied was to add a padding token to every phrase in order to be as long as the longest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8c56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, data, token_len):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized = tokenizer(data['text'], truncation=True, padding='max_length', max_length=token_len, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return {'text': tokenized, 'label': data['label']}\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, token_len):\n",
    "    ds_tokenized = dataset.map(lambda x: tokenize(tokenizer, x, token_len), batched=True)\n",
    "    return ClassificationDataset(ds_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dad2dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b6e6566441410da362f274685f0056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "ds_tokenized_train = tokenized_dataset(ds_train, tokenizer, TOKEN_LENGTH)\n",
    "ds_tokenized_test = tokenized_dataset(ds_test, tokenizer, TOKEN_LENGTH)\n",
    "if DATASET_STR == 'rotten_tomatoes':\n",
    "    ds_tokenized_val = tokenized_dataset(ds_val, tokenizer, TOKEN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae8a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_tokenized_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    ds_tokenized_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "if DATASET_STR == 'rotten_tomatoes':\n",
    "    dl_val = DataLoader(\n",
    "        ds_tokenized_val,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "for (x, y) in dl_train:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a75491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.lin(x)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.relu(logits)\n",
    "        return logits \n",
    "\n",
    "class TextClassificationLinear(nn.Module):\n",
    "    def __init__(self, token_length, n_embd, head_dim, depth=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.Linear(token_length, n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.lin_layers = nn.ModuleList([LinearLayer(n_embd) for _ in range(depth)])\n",
    "        self.head = nn.Linear(n_embd, head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.init_layer(x)\n",
    "        for i, layer in enumerate(self.lin_layers):\n",
    "            logits_skip = logits\n",
    "            logits = layer(logits)\n",
    "            if i % 2 == 0:\n",
    "                logits = logits + logits_skip\n",
    "        logits = self.head(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6714f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "head_dim = 2 if isinstance(loss_fn, torch.nn.CrossEntropyLoss) else 1\n",
    "check = 2\n",
    "model = TextClassificationLinear(\n",
    "    token_length=TOKEN_LENGTH,\n",
    "    n_embd=768,\n",
    "    head_dim=head_dim,\n",
    "    depth=5,\n",
    "    dropout=0.2\n",
    ").to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16751e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, dropout=0.2, pool=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.conv(x)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.relu(logits)\n",
    "        return F.max_pool1d(logits, kernel_size=2, stride=2) if self.pool else logits\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, depth, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.init_conv = ConvLayer(in_channel, out_channel, dropout, pool=True)\n",
    "        self.block = nn.ModuleList([ConvLayer(out_channel, out_channel, dropout) for _ in range(depth - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = x\n",
    "        logits = self.init_conv(logits)\n",
    "        for i, layer in enumerate(self.block):\n",
    "            logits_skip = logits\n",
    "            logits = layer(logits)\n",
    "            if i % 2 == 0:\n",
    "                logits = logits + logits_skip\n",
    "        return logits\n",
    "\n",
    "class TextClassificationConv(nn.Module):\n",
    "    def __init__(self, channels, depth, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([ConvBlock(channels[i], channels[i+1], depth, dropout) for i in range(len(channels)-1)])\n",
    "        self.head = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = x\n",
    "        for block in self.blocks:\n",
    "            logits = block(logits)\n",
    "        logits = self.head(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df911544",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "check = 2\n",
    "channels = [1, 64, 128, 256, 512, 1]\n",
    "model = TextClassificationConv(\n",
    "    channels=channels,\n",
    "    depth=5,\n",
    "    dropout=0.2\n",
    ").to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "561b8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Train Loss: 0.6931 Accuracy: 0.5000\n",
      "Validation Loss: 0.6931 Accuracy: 0.4980\n",
      "\n",
      "Epoch 4/20 Train Loss: 0.6931 Accuracy: 0.4999\n",
      "Validation Loss: 0.6931 Accuracy: 0.5000\n",
      "\n",
      "Epoch 6/20 Train Loss: 0.6931 Accuracy: 0.5001\n",
      "Validation Loss: 0.6931 Accuracy: 0.5040\n",
      "\n",
      "Epoch 8/20 Train Loss: 0.6931 Accuracy: 0.5000\n",
      "Validation Loss: 0.6931 Accuracy: 0.4960\n",
      "\n",
      "Epoch 10/20 Train Loss: 0.6931 Accuracy: 0.4999\n",
      "Validation Loss: 0.6931 Accuracy: 0.4960\n",
      "\n",
      "Epoch 12/20 Train Loss: 0.6931 Accuracy: 0.5001\n",
      "Validation Loss: 0.6931 Accuracy: 0.5040\n",
      "\n",
      "Epoch 14/20 Train Loss: 0.6931 Accuracy: 0.5001\n",
      "Validation Loss: 0.6931 Accuracy: 0.5020\n",
      "\n",
      "Epoch 16/20 Train Loss: 0.6931 Accuracy: 0.5001\n",
      "Validation Loss: 0.6931 Accuracy: 0.5000\n",
      "\n",
      "Epoch 18/20 Train Loss: 0.6931 Accuracy: 0.5001\n",
      "Validation Loss: 0.6931 Accuracy: 0.4980\n",
      "\n",
      "Epoch 20/20 Train Loss: 0.6931 Accuracy: 0.4999\n",
      "Validation Loss: 0.6931 Accuracy: 0.5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "    acc = 0\n",
    "    for (x, y) in dl_train:\n",
    "        x = x.to(torch.float).to(DEVICE) if isinstance(model, TextClassificationLinear) else x.to(torch.float).to(DEVICE).unsqueeze(1)\n",
    "        y = y.to(torch.long).to(DEVICE) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.to(torch.float).to(DEVICE)\n",
    "        logits = model(x) if isinstance(loss_fn, nn.CrossEntropyLoss) else model(x).flatten()\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_epoch += loss.item()\n",
    "        preds = torch.round(F.sigmoid(logits)) if isinstance(loss_fn, nn.BCEWithLogitsLoss) else torch.argmax(logits, dim=1)\n",
    "        acc += torch.sum(preds == y).item() / len(y)\n",
    "    loss_epoch /= len(dl_train)\n",
    "    acc /= len(dl_train)\n",
    "    \n",
    "    if (epoch+1) % check == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} Train Loss: {loss_epoch:.4f} Accuracy: {acc:.4f}\")\n",
    "        if DATASET_STR == 'rotten_tomatoes':\n",
    "            with torch.no_grad():\n",
    "                loss_val = 0\n",
    "                acc_val = 0\n",
    "                model.eval()\n",
    "                for (x, y) in dl_val:\n",
    "                    x = x.to(torch.float).to(DEVICE) if isinstance(model, TextClassificationLinear) else x.to(torch.float).to(DEVICE).unsqueeze(1)\n",
    "                    y = y.to(torch.long).to(DEVICE) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.to(torch.float).to(DEVICE)\n",
    "                    logits = model(x) if isinstance(loss_fn, nn.CrossEntropyLoss) else model(x).flatten()\n",
    "                    preds = torch.round(F.sigmoid(logits)) if isinstance(loss_fn, nn.BCEWithLogitsLoss) else torch.argmax(logits, dim=1)\n",
    "                    acc_val += torch.sum(preds == y).item() / len(y)\n",
    "                    loss = loss_fn(logits, y)\n",
    "                    loss_val += loss.item()\n",
    "                loss_val /= len(dl_val)\n",
    "                acc_val /= len(dl_val)\n",
    "                print(f\"Validation Loss: {loss_val:.4f} Accuracy: {acc_val:.4f}\")\n",
    "                print(\"\")\n",
    "model_str = \"conv\" if isinstance(model, TextClassificationConv) else \"linear\"\n",
    "torch.save(model.state_dict(), f\"text_classification_{model_str}_{DATASET_STR}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70a8f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6931 Accuracy: 0.5101\n",
      "Number of samples predicted correct: 533\n",
      "Number of samples predicted incorrect: 533\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "    inference = {'correct': 0, 'incorrect': 0}\n",
    "    for (x, y) in dl_test:\n",
    "        x = x.to(torch.float).to(DEVICE) if isinstance(model, TextClassificationLinear) else x.to(torch.float).to(DEVICE).unsqueeze(1)\n",
    "        y = y.to(torch.long).to(DEVICE) if isinstance(loss_fn, nn.CrossEntropyLoss) else y.to(torch.float).to(DEVICE)\n",
    "        logits = model(x) if isinstance(loss_fn, nn.CrossEntropyLoss) else model(x).flatten()\n",
    "        preds = torch.round(F.sigmoid(logits)) if isinstance(loss_fn, nn.BCEWithLogitsLoss) else torch.argmax(logits, dim=1)\n",
    "        acc_test += torch.sum(preds == y).item()/ len(y)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss_test += loss.item()\n",
    "        inference['correct'] += torch.sum(preds == y).item()\n",
    "        inference['incorrect'] += torch.sum(preds != y).item()\n",
    "    loss_test /= len(dl_test)\n",
    "    acc_test /= len(dl_test)\n",
    "    print(f\"Test Loss: {loss_test:.4f} Accuracy: {acc_test:.4f}\")\n",
    "    print(f\"Number of samples predicted correct: {inference['correct']}\")\n",
    "    print(f\"Number of samples predicted incorrect: {inference['incorrect']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e418dee",
   "metadata": {},
   "source": [
    "Sadly with both a linear and convolutional model I was not able to reach good performance, there are three main reasons: \n",
    "\n",
    "First is that the input distribution is not easy to learn the space of reviews contains lots of concepts a network should learn in order to be able to understand its content\n",
    "\n",
    "Second the padding is too much, this way I have lots (the majority) of samples made of padding tokens, these kind of samples does not carry enough information \n",
    "\n",
    "Third the model is too small or its architecture is too much simple to capture the underlying knowledge of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af761e06",
   "metadata": {},
   "source": [
    "### Hugging Face Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802731db",
   "metadata": {},
   "source": [
    "Hugging Face provides a framework to work with pretrained models and with lots of smart ideas like the DataCollator, a class with the purpose of dynamically pad or truncate during the fine tuning.\n",
    "\n",
    "Hugging Face provides a method to perform the finetuning a in easy way, giving the corresponding control parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5532c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorWithPadding, pipeline, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a34e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_STR = 'imdb' # 'rotten_tomatoes'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39038e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'stanfordnlp/imdb' if DATASET_STR == 'imdb' else 'cornell-movie-review-data/rotten_tomatoes'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40abe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    return tokenizer(data['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35145447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)\n",
    "tokenized_dataset = dataset.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589d6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e925fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97de99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c91b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f102d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "ds_train = tokenized_dataset[\"train\"]\n",
    "\n",
    "for i in range(len(ds_train)):\n",
    "    print(f\"Text: {ds_train[i]['text']}\")\n",
    "    print(f\"Label: {ds_train[i]['label']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad18ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"hf_model/model_{DATASET_STR}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    #use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbc394",
   "metadata": {},
   "source": [
    "I defined 4 category of test reviews: absolute positive, absolute negative, sarcastic positive, sarcastic negative\n",
    "\n",
    "The idea is that to predict the first two should be easier than the others, but if we are able to reach the correct classification for the sarcastic examples then we could infer that the model has learnt the context and has distilled the distribution knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cefb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_text_pos = \"This movie was great!\"\n",
    "inference_text_neg = \"This movie was terrible!\"\n",
    "inference_text_sarc1 = \"I hate the love story in this movie, but I love the action scenes!\"\n",
    "inference_text_sarc2 = \"I hate this movie as much as I love Lord of the Rings!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7d169b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 3126 if DATASET_STR == 'imdb' else 1068\n",
    "model_path = f\"hf_model/model_{DATASET_STR}/checkpoint-{checkpoint}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3012010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9959372282028198}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9936391711235046}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9964547157287598}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.6173900961875916}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "print(classifier(inference_text_pos))\n",
    "print(classifier(inference_text_neg))\n",
    "print(classifier(inference_text_sarc1))\n",
    "print(classifier(inference_text_sarc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d981123",
   "metadata": {},
   "source": [
    "As we can see the predictions are awesomes, the model has very high scores, the only peculiar fact is the prediction of the negative sarcastic review which has a much lower confidence, due to the dipendence of the review on a subjective topic: the liking of another movie, still the prediction is correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
